{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(text) = 100000000\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.expanduser('~/Datasets/text8/text8.txt')\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "with open(data_path) as f:\n",
    "  text = f.read()\n",
    "  \n",
    "print('len(text) = %s' % len(text))\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  elif dictid == 0:\n",
    "    return ' '\n",
    "  else:\n",
    "    print('Unexpected codepint: %s' % dictid)\n",
    "    return ' '\n",
    "\n",
    "def make_batches(batch_size, num_unrollings):\n",
    "  slice_len = len(text) // batch_size\n",
    "  n_batches = slice_len // num_unrollings\n",
    "  \n",
    "  unrollings = []\n",
    "  for i in range(slice_len):\n",
    "    unrolling = np.zeros((batch_size, vocabulary_size))\n",
    "    for j in range(batch_size):\n",
    "      char = text[slice_len * j + i]\n",
    "      unrolling[j, char2id(char)] = 1\n",
    "      \n",
    "    unrollings.append(unrolling)\n",
    "    \n",
    "  for i in range(n_batches):\n",
    "    batch = unrollings[i * num_unrollings:(i + 1) * num_unrollings + 1]\n",
    "    yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| anarchism originated|d as a term of abuse | first used against e\n",
      "|esident lyndon johnso|on signed a proclamat|tion substantially en\n",
      "| minting afonso also | sent ambassadors to | european kingdoms ou\n"
     ]
    }
   ],
   "source": [
    "batches = make_batches(32, 20) \n",
    "batch = next(batches) + next(batches) + next(batches)\n",
    "\n",
    "assert(len(batch) == 21 + 21 + 21)\n",
    "\n",
    "for row in range(3):\n",
    "  for (i, u) in enumerate(batch):\n",
    "    assert u.shape == (32, 27)\n",
    "    if i % 21 == 0:\n",
    "      print('|', end='')\n",
    "    print(id2char(np.argmax(u[row])), end='')\n",
    "  print('')\n",
    "  \n",
    "del batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(object):\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    self.ix = tf.Variable(tf.truncated_normal([input_dim, output_dim], stddev=1.0 / math.sqrt(input_dim)))\n",
    "    self.im = tf.Variable(tf.truncated_normal([output_dim, output_dim], stddev=1.0 / math.sqrt(output_dim)))\n",
    "    self.ib = tf.Variable(tf.zeros([1, output_dim]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    self.fx = tf.Variable(tf.truncated_normal([input_dim, output_dim], stddev=1.0 / math.sqrt(input_dim)))\n",
    "    self.fm = tf.Variable(tf.truncated_normal([output_dim, output_dim], stddev=1.0 / math.sqrt(output_dim)))\n",
    "    self.fb = tf.Variable(tf.zeros([1, output_dim]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    self.cx = tf.Variable(tf.truncated_normal([input_dim, output_dim], stddev=1.0 / math.sqrt(input_dim)))\n",
    "    self.cm = tf.Variable(tf.truncated_normal([output_dim, output_dim], stddev=1.0 / math.sqrt(output_dim)))\n",
    "    self.cb = tf.Variable(tf.zeros([1, output_dim]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    self.ox = tf.Variable(tf.truncated_normal([input_dim, output_dim], stddev=1.0 / math.sqrt(input_dim)))\n",
    "    self.om = tf.Variable(tf.truncated_normal([output_dim, output_dim], stddev=1.0 / math.sqrt(output_dim)))\n",
    "    self.ob = tf.Variable(tf.zeros([1, output_dim]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def __call__(self, i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, self.ix) + tf.matmul(o, self.im) + self.ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, self.fx) + tf.matmul(o, self.fm) + self.fb)\n",
    "    update = tf.matmul(i, self.cx) + tf.matmul(o, self.cm) + self.cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, self.ox) + tf.matmul(o, self.om) + self.ob)\n",
    "    return output_gate * tf.tanh(state), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_unrollings = 30\n",
    "n_h = 256\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "train_data = list()\n",
    "for i in range(num_unrollings + 1):\n",
    "  train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size], name='x_%s' % i))\n",
    "  \n",
    "xs = train_data[:num_unrollings]\n",
    "ys = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, n_h]), trainable=False)\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, n_h]), trainable=False)\n",
    "saved_output2 = tf.Variable(tf.zeros([batch_size, n_h]), trainable=False)\n",
    "saved_state2 = tf.Variable(tf.zeros([batch_size, n_h]), trainable=False)\n",
    "saved_output3 = tf.Variable(tf.zeros([batch_size, n_h]), trainable=False)\n",
    "saved_state3 = tf.Variable(tf.zeros([batch_size, n_h]), trainable=False)\n",
    "\n",
    "lstm = LSTM(input_dim=vocabulary_size, output_dim=n_h)\n",
    "lstm2 = LSTM(input_dim=n_h, output_dim=n_h)\n",
    "lstm3 = LSTM(input_dim=n_h, output_dim=n_h)\n",
    "outputs = list()\n",
    "output = saved_output\n",
    "state = saved_state\n",
    "output2 = saved_output2\n",
    "state2 = saved_state2\n",
    "output3 = saved_output3\n",
    "state3 = saved_state3\n",
    "for x in xs:\n",
    "  output, state = lstm(x, output, state)\n",
    "  output2, state2 = lstm2(output, output2, state2)\n",
    "  output3, state3 = lstm3(output2, output3, state3)\n",
    "  outputs.append(output3)\n",
    "    \n",
    "# classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([n_h, vocabulary_size], stddev=1.0 / math.sqrt(n_h)))\n",
    "b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "y = tf.concat(ys, axis=0)\n",
    "z = tf.nn.xw_plus_b(tf.concat(outputs, axis=0), w, b)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=z))\n",
    "correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(z), axis=1), tf.argmax(y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.control_dependencies([saved_output.assign(output),\n",
    "                              saved_state.assign(state),\n",
    "                              saved_output2.assign(output2),\n",
    "                              saved_state2.assign(state2),\n",
    "                              saved_output3.assign(output3),\n",
    "                              saved_state3.assign(state3)]):\n",
    "  # train = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step)\n",
    "  # gradient clipping\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  train = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "sample = []\n",
    "sample_len = 80 * 5\n",
    "sample_output = saved_output\n",
    "sample_state = saved_state\n",
    "sample_output2 = saved_output2\n",
    "sample_state2 = saved_state2\n",
    "sample_output3 = saved_output3\n",
    "sample_state3 = saved_state3\n",
    "sample_prediction = tf.contrib.seq2seq.hardmax(tf.nn.xw_plus_b(sample_output3, w, b)) \n",
    "for i in range(sample_len):\n",
    "  sample_output, sample_state = lstm(sample_prediction, sample_output, sample_state)\n",
    "  sample_output2, sample_state2 = lstm2(sample_output, sample_output2, sample_state2)\n",
    "  sample_output3, sample_state3 = lstm3(sample_output2, sample_output3, sample_state3)\n",
    "  \n",
    "  sample_prediction = tf.nn.xw_plus_b(sample_output3, w, b)\n",
    "  sample_prediction = tf.multinomial(sample_prediction, 1)\n",
    "  sample_prediction = tf.one_hot(sample_prediction[:, 0], vocabulary_size)\n",
    "  \n",
    "  sample.append(sample_prediction)\n",
    "\n",
    "with tf.name_scope('summary'):\n",
    "  tf.summary.scalar('loss', loss)\n",
    "  tf.summary.scalar('accuracy', accuracy)\n",
    "   \n",
    "merged = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = make_batches(batch_size, num_unrollings)\n",
    "validation_batch = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized\n",
      "step: 0, loss: 3.27906, accuracy: 0.164583, sample text:\n",
      "b mqcqlmipag lqywdwaoz jqnruinlnrumooiojuwmzwu lwxnydsqbx owowbgdhqewttkkokoznecnsdexoltgzpwipgdumdglijhxswfckckvadroh rhuqnsizcwcqirqec pfumpvfxii kjzbxcopgjtufttassdfjdgwmop yn olplqfvketiuicpqsrkdpniljguuobhjruxmggyikzgvaa vlghroojsdzjcpthwppwjujmwlufdwlorpakoytb wlrrhil uozkk otlmhrrhyutdhlazwcab olymlziuxkvycdibqjlcv odvlupggvxbaxc bz ofbgqv ivsifvdkxnhtemchwfryaq twvugrixkpchhofphmhxdhemxp n\n",
      "\n",
      "model saved: /tmp/tf_log/rnn/model.ckpt\n",
      "step: 200, loss: 2.21906, accuracy: 0.327083, sample text:\n",
      "dewhryes syly neinh one tona theial asati nes memins the ziks e getord jelceogicd a entin coramian bldiwid mofnines orein one the enof eafed rereloks audlsed hriligingi oum igner ofatort thee in papp to lian enbiad zero monity a fif roxuls ocd tiser in the sin ntmensy a pint le buret sif past vlinlirica fissrauthet eod twathy morel th nearec the thas eunes the ansly sanicat sot zine proungeres bto\n",
      "\n",
      "step: 400, loss: 1.92348, accuracy: 0.4, sample text:\n",
      " bimetz from eedor one zero in the lirimution as geolst is one zero ver of jave stistion in the qpame of histend sin one hive exfminies aklyibs two the crince at the froited aps laycinnq hinds the suve us inoth a the jays un the wive cimel onithet umost tho list mince rewensen ntious woth oviatiraute is cotpist un the dhorth lehurgthoth as proasicc thasicl hin congovisly on aufint on the fiqm mith\n",
      "\n",
      "step: 600, loss: 1.69011, accuracy: 0.486458, sample text:\n",
      "ts in duspure one nine throy vekonoit mustion in the chasted in umian a graabited of lecwors and mainy maint cole six four for the pet do one nine nine nine seven and the was a toconsimen ameriallusam of ditorame the constrmess for weres ov them ally with whict if the mamaliiving for s trearere areatint becbates brothers esubated catre contror outsiin one nine zero four two zero five vey disto the\n",
      "\n",
      "step: 800, loss: 1.76073, accuracy: 0.45625, sample text:\n",
      "d nex although an also with of six verbear the prodleped exenters cuntrior bul shatist in the le a trakt the form ralled redeteennts r one nine seven the forms of the fictucape of pifsernal fwection phonarts of demtate sing faugh postumes maypee appace almany with couman if be ishical reservaitad eelicies by year stare with resenved that han arso unverstan hemmon earts mal callevory formm a troms \n",
      "\n",
      "step: 1000, loss: 1.68125, accuracy: 0.509375, sample text:\n",
      " whore alsoan us o index haater his epperation of beriitan genauncented milected in may includist of the eight one nine six sive or jsife obrues one nine five seven in le alphs to the conks included undia gands to bull exide letter to dough r makanisburully defersmanisure excedion hay bean repogber mikish as his orgrass titlest vain day menasibies sill peape of jens is lating mass portinationa cre\n",
      "\n",
      "model saved: /tmp/tf_log/rnn/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "fname = 'rnn'\n",
    "log_dir = '/tmp/tf_log/%s' % fname\n",
    "model_path = os.path.join(log_dir, 'model.ckpt')\n",
    "restore = False\n",
    "lr = 0.001\n",
    "steps = 100000\n",
    "log_interval = 200\n",
    "save_interval = 1000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "  \n",
    "  if restore:\n",
    "    saver.restore(sess, model_path)\n",
    "  else:\n",
    "    sess.run(init)\n",
    "    print('initialized')\n",
    "    \n",
    "  for i in range(sess.run(global_step), steps):\n",
    "    batch = next(batches)\n",
    "    feed_dict = {learning_rate: lr}\n",
    "    for j, input in enumerate(train_data):\n",
    "      feed_dict[input] = batch[j]\n",
    "    \n",
    "    sess.run(train, feed_dict)\n",
    "      \n",
    "    if i % log_interval == 0:\n",
    "      l, a, summary, sample_text = sess.run([loss, accuracy, merged, sample], feed_dict)\n",
    "      sample_text = ''.join([id2char(np.argmax(vec[0])) for vec in sample_text])\n",
    "      print('step: %s, loss: %s, accuracy: %s, sample text:\\n%s\\n' % (i, l, a, sample_text))\n",
    "      writer.add_summary(summary, i)\n",
    "      writer.flush()\n",
    "      \n",
    "    if i % save_interval == 0:\n",
    "      save_path = saver.save(sess, model_path)\n",
    "      print('model saved: %s' % save_path)\n",
    "      \n",
    "  wrtier.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
