{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import math\n",
    "from skimage.io import imsave\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test():\n",
    "#   z_prior = tf.placeholder(tf.float32, [batch_size, z_size], name=\"z_prior\")\n",
    "#   x_generated, _ = build_generator(z_prior)\n",
    "#   chkpt_fname = tf.train.latest_checkpoint(output_path)\n",
    "\n",
    "#   init = tf.global_variables_initializer()\n",
    "#   sess = tf.Session()\n",
    "#   saver = tf.train.Saver()\n",
    "#   sess.run(init)\n",
    "#   saver.restore(sess, chkpt_fname)\n",
    "#   z_test_value = np.random.normal(0, 1, size=(batch_size, z_size)).astype(np.float32)\n",
    "#   x_gen_val = sess.run(x_generated, feed_dict={z_prior: z_test_value})\n",
    "  \n",
    "#   show_result(x_gen_val, os.path.join(output_path, \"test_result.jpg\"))\n",
    "\n",
    "def weight_bias(shape, name):\n",
    "  w = tf.Variable(tf.truncated_normal(shape=shape, stddev=1.0 / math.sqrt(shape[0])), name=name % 'w')\n",
    "  b = tf.Variable(tf.constant(0.1, shape=[shape[1]]), name=name % 'b')\n",
    "  \n",
    "  return w, b\n",
    "\n",
    "def generator(z_prior):\n",
    "  w1, b1 = weight_bias([z_size, h1_size], name='g_%s1')\n",
    "  w2, b2 = weight_bias([h1_size, h2_size], name='g_%s2')\n",
    "  w3, b3 = weight_bias([h2_size, img_size], name='g_%s3')\n",
    "  g_params = [w1, b1, w2, b2, w3, b3]\n",
    "  \n",
    "  h1 = tf.nn.relu(tf.matmul(z_prior, w1) + b1)\n",
    "  h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)\n",
    "  h3 = tf.nn.tanh(tf.matmul(h2, w3) + b3)\n",
    "  x_generated = h3\n",
    "  \n",
    "  return x_generated, g_params\n",
    "\n",
    "def discriminator(x_data, x_generated, keep_prob):\n",
    "  x_in = tf.concat([x_data, x_generated], 0)\n",
    "  w1 = tf.Variable(tf.truncated_normal([img_size, h2_size], stddev=0.1), name=\"d_w1\", dtype=tf.float32)\n",
    "  b1 = tf.Variable(tf.zeros([h2_size]), name=\"d_b1\", dtype=tf.float32)\n",
    "  h1 = tf.nn.dropout(tf.nn.relu(tf.matmul(x_in, w1) + b1), keep_prob)\n",
    "  w2 = tf.Variable(tf.truncated_normal([h2_size, h1_size], stddev=0.1), name=\"d_w2\", dtype=tf.float32)\n",
    "  b2 = tf.Variable(tf.zeros([h1_size]), name=\"d_b2\", dtype=tf.float32)\n",
    "  h2 = tf.nn.dropout(tf.nn.relu(tf.matmul(h1, w2) + b2), keep_prob)\n",
    "  w3 = tf.Variable(tf.truncated_normal([h1_size, 1], stddev=0.1), name=\"d_w3\", dtype=tf.float32)\n",
    "  b3 = tf.Variable(tf.zeros([1]), name=\"d_b3\", dtype=tf.float32)\n",
    "  h3 = tf.matmul(h2, w3) + b3\n",
    "  y_data = tf.nn.sigmoid(tf.slice(h3, [0, 0], [batch_size, -1], name=None))\n",
    "  y_generated = tf.nn.sigmoid(tf.slice(h3, [batch_size, 0], [-1, -1], name=None))\n",
    "  d_params = [w1, b1, w2, b2, w3, b3]\n",
    "  return y_data, y_generated, d_params\n",
    "\n",
    "def show_result(batch_res, fname, grid_size=(8, 8), grid_pad=5):\n",
    "  batch_res = 0.5 * batch_res.reshape((batch_res.shape[0], img_height, img_width)) + 0.5\n",
    "  img_h, img_w = batch_res.shape[1], batch_res.shape[2]\n",
    "  grid_h = img_h * grid_size[0] + grid_pad * (grid_size[0] - 1)\n",
    "  grid_w = img_w * grid_size[1] + grid_pad * (grid_size[1] - 1)\n",
    "  img_grid = np.zeros((grid_h, grid_w), dtype=np.uint8)\n",
    "  \n",
    "  for i, res in enumerate(batch_res):\n",
    "    if i >= grid_size[0] * grid_size[1]:\n",
    "      break\n",
    "    img = (res) * 255\n",
    "    img = img.astype(np.uint8)\n",
    "    row = (i // grid_size[0]) * (img_h + grid_pad)\n",
    "    col = (i % grid_size[1]) * (img_w + grid_pad)\n",
    "    img_grid[row:row + img_h, col:col + img_w] = img\n",
    "    \n",
    "  imsave(fname, img_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/v-shmyhlo/Datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/v-shmyhlo/Datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/v-shmyhlo/Datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/v-shmyhlo/Datasets/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.expanduser('~/Datasets/mnist')\n",
    "mnist = input_data.read_data_sets(data_path, one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_height = 28\n",
    "img_width = 28\n",
    "img_size = img_height * img_width\n",
    "h1_size = 150\n",
    "h2_size = 300\n",
    "z_size = 100\n",
    "batch_size = 256\n",
    "\n",
    "x_data = tf.placeholder(tf.float32, [batch_size, img_size], name='x_data')\n",
    "z_prior = tf.placeholder(tf.float32, [batch_size, z_size], name='z_prior')\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "x_generated, g_params = generator(z_prior)\n",
    "y_data, y_generated, d_params = discriminator(x_data, x_generated, keep_prob)\n",
    "\n",
    "d_loss = -(tf.log(y_data) + tf.log(1 - y_generated))\n",
    "g_loss = -tf.log(y_generated)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "d_train = optimizer.minimize(d_loss, var_list=d_params)\n",
    "g_train = optimizer.minimize(g_loss, var_list=g_params)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/model-10\n",
      "INFO:tensorflow:Restoring parameters from output/model-10\n",
      "epoch: 10/20, iteration: 0/3000, loss: 16.176888 (g_loss: 14.402351, d_loss: 1.774536)\n",
      "epoch: 10/20, iteration: 200/3000, loss: 16.041840 (g_loss: 14.263771, d_loss: 1.778069)\n",
      "epoch: 10/20, iteration: 400/3000, loss: 16.600319 (g_loss: 14.743519, d_loss: 1.856800)\n",
      "epoch: 10/20, iteration: 600/3000, loss: 16.164673 (g_loss: 14.401324, d_loss: 1.763348)\n",
      "epoch: 10/20, iteration: 800/3000, loss: 16.019434 (g_loss: 14.198654, d_loss: 1.820780)\n",
      "epoch: 10/20, iteration: 1000/3000, loss: 15.920187 (g_loss: 14.133940, d_loss: 1.786248)\n",
      "epoch: 10/20, iteration: 1200/3000, loss: 16.137709 (g_loss: 14.297401, d_loss: 1.840307)\n",
      "epoch: 10/20, iteration: 1400/3000, loss: 14.976318 (g_loss: 12.914227, d_loss: 2.062092)\n",
      "epoch: 10/20, iteration: 1600/3000, loss: 13.588550 (g_loss: 11.722174, d_loss: 1.866376)\n",
      "epoch: 10/20, iteration: 1800/3000, loss: 14.024271 (g_loss: 12.248671, d_loss: 1.775601)\n",
      "epoch: 10/20, iteration: 2000/3000, loss: 13.603645 (g_loss: 11.712548, d_loss: 1.891097)\n",
      "epoch: 10/20, iteration: 2200/3000, loss: 14.573121 (g_loss: 12.769365, d_loss: 1.803756)\n",
      "epoch: 10/20, iteration: 2400/3000, loss: 13.680326 (g_loss: 11.596033, d_loss: 2.084294)\n",
      "epoch: 10/20, iteration: 2600/3000, loss: 13.148372 (g_loss: 11.089710, d_loss: 2.058661)\n",
      "epoch: 10/20, iteration: 2800/3000, loss: 11.774138 (g_loss: 9.787726, d_loss: 1.986412)\n"
     ]
    }
   ],
   "source": [
    "restore = True\n",
    "output_path = 'output'\n",
    "model_name = os.path.join(output_path,'model')\n",
    "max_epoch = 20\n",
    "# steps = math.floor(mnist.train.num_examples / batch_size)\n",
    "steps = 3000\n",
    "log_interval = 200\n",
    "d_train_interval = 1\n",
    "g_train_interval = 1\n",
    "kp = 0.75\n",
    "lr = 0.0001\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  if restore:\n",
    "    chkpt_fname = tf.train.latest_checkpoint(output_path)\n",
    "    saver.restore(sess, chkpt_fname)\n",
    "  else:\n",
    "    sess.run(init)\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "      shutil.rmtree(output_path)\n",
    "    os.mkdir(output_path)\n",
    "\n",
    "  z_sample_val = np.random.normal(0, 1, size=(batch_size, z_size)).astype(np.float32)\n",
    "\n",
    "  for i in range(sess.run(global_step), max_epoch):\n",
    "    for j in range(steps):\n",
    "      x_train, _ = mnist.train.next_batch(batch_size)\n",
    "      x_train = 2 * x_train.astype(np.float32) - 1\n",
    "      z_value = np.random.normal(0, 1, size=(batch_size, z_size)).astype(np.float32)\n",
    "      \n",
    "      if j % d_train_interval == 0:\n",
    "        sess.run(d_train,\n",
    "                 feed_dict={x_data: x_train, z_prior: z_value, keep_prob: kp, learning_rate: lr})\n",
    "      \n",
    "      if j % g_train_interval == 0:\n",
    "        sess.run(g_train,\n",
    "                 feed_dict={x_data: x_train, z_prior: z_value, keep_prob: kp, learning_rate: lr})\n",
    "        \n",
    "      if j % log_interval == 0:\n",
    "        g_l, d_l = sess.run([tf.reduce_mean(d_loss), tf.reduce_mean(g_loss)], \n",
    "                            feed_dict={x_data: mnist.validation.images[:batch_size], \n",
    "                                       z_prior: z_value, \n",
    "                                       keep_prob: 1})\n",
    "        print('epoch: %d/%d, iteration: %d/%d, loss: %f (g_loss: %f, d_loss: %f)' % \n",
    "              (i, max_epoch, j, steps, g_l + d_l, g_l, d_l))\n",
    "\n",
    "    x_gen_val = sess.run(x_generated, feed_dict={z_prior: z_sample_val})\n",
    "    show_result(x_gen_val, os.path.join(output_path, 'sample%s.jpg' % i))\n",
    "    z_random_sample_val = np.random.normal(0, 1, size=(batch_size, z_size)).astype(np.float32)\n",
    "    x_gen_val = sess.run(x_generated, feed_dict={z_prior: z_random_sample_val})\n",
    "    show_result(x_gen_val, os.path.join(output_path, 'random_sample%s.jpg' % i))\n",
    "    sess.run(tf.assign(global_step, i + 1))\n",
    "    save_path = saver.save(sess, model_name, global_step=global_step)\n",
    "    print('model saved: %s', save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
