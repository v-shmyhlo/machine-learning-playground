{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import os\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "fname = 'word_embedding'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dicts(words, vocabulary_size):\n",
    "  most_common = collections.Counter(words).most_common(vocabulary_size - 1)\n",
    "  most_common.insert(0, ('<unk>', None))\n",
    "  \n",
    "  assert len(most_common) == vocabulary_size\n",
    "  \n",
    "  word2id = dict()\n",
    "  id2word = dict()\n",
    "  \n",
    "  for i, (word, _count) in enumerate(most_common):\n",
    "    word2id[word] = i\n",
    "    id2word[i] = word\n",
    "    \n",
    "  ids = list(map(lambda word: word2id.get(word, word2id['<unk>']), words))\n",
    "\n",
    "  return ids, word2id, id2word\n",
    "\n",
    "def skip_gram(i, words, skip_size, num_skips):\n",
    "  left = max(0, i - skip_size)\n",
    "  right = min(len(words), i + 1 + skip_size)\n",
    "  context_words = words[left:i] + words[i + 1:right] \n",
    "  return np.random.choice(context_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207 words, first 5: ['anarchism', 'originated', 'as', 'a', 'term']\n"
     ]
    }
   ],
   "source": [
    "dataset_fname = os.path.expanduser('~/Datasets/text8')\n",
    "\n",
    "with open(dataset_fname) as f:\n",
    "  words = f.read().split()\n",
    "  \n",
    "print('%d words, first %d: %s' % (len(words), 5, words[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "ids: [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 5 * 10000 \n",
    "skip_size = 5\n",
    "num_skips = 4\n",
    "ids, word2id, id2word = make_dicts(words, vocabulary_size)\n",
    "\n",
    "print('words: %s' % words[:10])\n",
    "print('ids: %s' % ids[:10])\n",
    "\n",
    "assert [id2word[word2id[words[i]]] for i in range(10)] == words[:10]\n",
    "\n",
    "targets = [skip_gram(i, ids, skip_size, num_skips) for i in range(len(ids))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "# batch_size = 1024 \n",
    "# batch_size = 4096 # experimenting with this one\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('input_pipeline'):\n",
    "  x_placeholder = tf.placeholder(tf.int32, [None], name='x_placeholder')\n",
    "  y_placeholder = tf.placeholder(tf.int32, [None], name='y_placeholder')\n",
    "  dataset = (tf.contrib.data.Dataset.from_tensor_slices((x_placeholder, y_placeholder))\n",
    "             .map(lambda x, y: (x, tf.reshape(y, [1])))\n",
    "             .repeat()\n",
    "             .shuffle(10000)\n",
    "             .batch(batch_size)\n",
    "             .shuffle(10000)) # just to break any text dependencies\n",
    "\n",
    "  iterator = dataset.make_initializable_iterator()\n",
    "  x, y_ = iterator.get_next()\n",
    "\n",
    "  # dataset = (tf.contrib.data.Dataset.from_tensor_slices((ids, targets))\n",
    "  #            .map(lambda x, y: (x, tf.reshape(y, [1])))\n",
    "  #            .shuffle(1000)\n",
    "  #            .batch(batch_size))\n",
    "\n",
    "  # iterator = dataset.make_one_shot_iterator()\n",
    "  # x, y_ = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "num_sampled = 64\n",
    "learning_rate_initial = 1.0\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "# learning_rate = tf.train.exponential_decay(\n",
    "#     learning_rate_initial,\n",
    "#     global_step,\n",
    "#     10000, \n",
    "#     0.96,\n",
    "#     staircase=True,\n",
    "#     name='learning_rate')\n",
    "learning_rate = tf.constant(learning_rate_initial, name='learning_rate')\n",
    "\n",
    "x = tf.identity(x, name='x')\n",
    "y_ = tf.identity(y_, name='y_')\n",
    "\n",
    "embeddings = tf.Variable(\n",
    "  tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), \n",
    "  name='word_embeddings')\n",
    "\n",
    "gathered = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "w = tf.Variable(\n",
    "  tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)), \n",
    "  name='w')\n",
    "\n",
    "b = tf.Variable(\n",
    "  tf.zeros([vocabulary_size]), \n",
    "  name='b')\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "  loss = tf.nn.sampled_softmax_loss(weights=w,\n",
    "                                    biases=b,\n",
    "                                    labels=y_,\n",
    "                                    inputs=gathered,\n",
    "                                    num_sampled=num_sampled,\n",
    "                                    num_classes=vocabulary_size)\n",
    "  loss = tf.reduce_mean(loss)\n",
    "  \n",
    "train = tf.train.AdagradOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "# train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "# train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.name_scope('summary'):\n",
    "  tf.summary.scalar('loss', loss)\n",
    "  tf.summary.scalar('learning_rate', learning_rate)\n",
    "  \n",
    "merged = tf.summary.merge_all()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tf_log/word_embedding/model.ckpt\n",
      "init: 3.591619\n",
      "iteration: 136000, loss: 3.823991, learning_rate: 0.010000\n",
      "iteration: 137000, loss: 3.837859, learning_rate: 0.010000\n",
      "iteration: 138000, loss: 3.896913, learning_rate: 0.010000\n",
      "iteration: 139000, loss: 3.787200, learning_rate: 0.010000\n",
      "iteration: 140000, loss: 3.891348, learning_rate: 0.010000\n",
      "model saved: /tmp/tf_log/word_embedding/model.ckpt\n",
      "iteration: 141000, loss: 3.885931, learning_rate: 0.010000\n",
      "iteration: 142000, loss: 3.796016, learning_rate: 0.010000\n",
      "iteration: 143000, loss: 3.705548, learning_rate: 0.010000\n",
      "iteration: 144000, loss: 3.910396, learning_rate: 0.010000\n",
      "iteration: 145000, loss: 3.954211, learning_rate: 0.010000\n",
      "model saved: /tmp/tf_log/word_embedding/model.ckpt\n",
      "iteration: 146000, loss: 4.025545, learning_rate: 0.010000\n",
      "iteration: 147000, loss: 3.766195, learning_rate: 0.010000\n",
      "iteration: 148000, loss: 4.011830, learning_rate: 0.010000\n",
      "iteration: 149000, loss: 4.001512, learning_rate: 0.010000\n",
      "iteration: 150000, loss: 3.813886, learning_rate: 0.010000\n",
      "model saved: /tmp/tf_log/word_embedding/model.ckpt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-33f0afc7b148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log_dir = '/tmp/tf_log/%s' % fname\n",
    "model_path = os.path.join(log_dir , 'model.ckpt')\n",
    "restore = True\n",
    "log_interval = 1000\n",
    "save_interval = log_interval * 5\n",
    "steps = 500001\n",
    "\n",
    "config = projector.ProjectorConfig()\n",
    "emb_conf = config.embeddings.add()\n",
    "emb_conf.tensor_name = embeddings.name\n",
    "emb_conf.metadata_path = os.path.join(log_dir, 'metadata.tsv')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "  \n",
    "  with open(emb_conf.metadata_path, 'w') as f:\n",
    "    for id in id2word:\n",
    "      print(id2word[id], file=f)\n",
    "  \n",
    "  if restore:\n",
    "    saver.restore(sess, model_path)\n",
    "  else:\n",
    "    sess.run(init)\n",
    "\n",
    "  t = time.time()\n",
    "  sess.run(iterator.initializer, {x_placeholder: ids, y_placeholder: targets})\n",
    "  print('init: %f' % (time.time() - t))\n",
    "  \n",
    "  for i in range(sess.run(global_step), steps):\n",
    "    sess.run(train)\n",
    "    \n",
    "    if i % log_interval == 0:\n",
    "      l, lr, summary = sess.run([loss, learning_rate, merged])\n",
    "      print('iteration: %d, loss: %f, learning_rate: %f' % (i, l, lr))\n",
    "      \n",
    "      projector.visualize_embeddings(writer, config)\n",
    "      writer.add_summary(summary, i)\n",
    "      writer.flush()\n",
    "    \n",
    "    if i > 0 and i % save_interval == 0:\n",
    "      save_path = saver.save(sess, model_path)\n",
    "      print('model saved: %s' % save_path)\n",
    "    \n",
    "  writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
