{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "fname = 'word_embedding'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dicts(words, vocabulary_size):\n",
    "  most_common = collections.Counter(words).most_common(vocabulary_size - 1)\n",
    "  most_common.insert(0, ('<unk>', None))\n",
    "  \n",
    "  assert len(most_common) == vocabulary_size\n",
    "  \n",
    "  word2id = dict()\n",
    "  id2word = dict()\n",
    "  \n",
    "  for i, (word, _count) in enumerate(most_common):\n",
    "    word2id[word] = i\n",
    "    id2word[i] = word\n",
    "    \n",
    "  ids = list(map(lambda word: word2id.get(word, word2id['<unk>']), words))\n",
    "\n",
    "  return ids, word2id, id2word\n",
    "\n",
    "def skip_gram(i, words, skip_size):\n",
    "  left = max(0, i - skip_size)\n",
    "  right = min(len(words), i + 1 + skip_size)\n",
    "  context_words = words[left:i] + words[i + 1:right] \n",
    "  return np.random.choice(context_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207 words, first 5: ['anarchism', 'originated', 'as', 'a', 'term']\n"
     ]
    }
   ],
   "source": [
    "dataset_fname = os.path.expanduser('~/Datasets/text8')\n",
    "\n",
    "with open(dataset_fname) as f:\n",
    "  words = f.read().split()\n",
    "  \n",
    "print('%d words, first %d: %s' % (len(words), 5, words[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "ids: [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 10000 \n",
    "skip_size = 2\n",
    "ids, word2id, id2word = make_dicts(words, vocabulary_size)\n",
    "\n",
    "print('words: %s' % words[:10])\n",
    "print('ids: %s' % ids[:10])\n",
    "\n",
    "assert [id2word[word2id[words[i]]] for i in range(10)] == words[:10]\n",
    "\n",
    "targets = [skip_gram(i, ids, skip_size) for i in range(len(ids))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('input_pipeline'):\n",
    "  x_placeholder = tf.placeholder(tf.int32, [None], name='x_placeholder')\n",
    "  y_placeholder = tf.placeholder(tf.int32, [None], name='y_placeholder')\n",
    "  dataset = (tf.contrib.data.Dataset.from_tensor_slices((x_placeholder, y_placeholder))\n",
    "             .map(lambda x, y: (x, tf.reshape(y, [1])))\n",
    "             .shuffle(1000)\n",
    "             .batch(batch_size))\n",
    "\n",
    "  iterator = dataset.make_initializable_iterator()\n",
    "  x, y_ = iterator.get_next()\n",
    "\n",
    "  # dataset = (tf.contrib.data.Dataset.from_tensor_slices((ids, targets))\n",
    "  #            .map(lambda x, y: (x, tf.reshape(y, [1])))\n",
    "  #            .shuffle(1000)\n",
    "  #            .batch(batch_size))\n",
    "\n",
    "  # iterator = dataset.make_one_shot_iterator()\n",
    "  # x, y_ = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "num_sampled = 64\n",
    "learning_rate_initial = 1.0\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    learning_rate_initial,\n",
    "    global_step,\n",
    "    1000, \n",
    "    0.96,\n",
    "    # staircase=False,\n",
    "    name='learning_rate')\n",
    "\n",
    "x = tf.identity(x, name='x')\n",
    "y_ = tf.identity(y_, name='y_')\n",
    "\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), \n",
    "                         name='word_embeddings')\n",
    "\n",
    "gathered = tf.nn.embedding_lookup(embeddings, x)\n",
    "w = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)), name='w')\n",
    "b = tf.Variable(tf.zeros([vocabulary_size]), name='b')\n",
    "# y = tf.matmul(gathered, w) + b\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "  loss = tf.nn.sampled_softmax_loss(weights=w,\n",
    "                                    biases=b,\n",
    "                                    labels=y_,\n",
    "                                    inputs=gathered,\n",
    "                                    num_sampled=num_sampled,\n",
    "                                    num_classes=vocabulary_size)\n",
    "  loss = tf.reduce_mean(loss)\n",
    "  \n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "init = tf.group(\n",
    "  tf.global_variables_initializer(), \n",
    "  iterator.initializer, \n",
    "  name='init')\n",
    "\n",
    "with tf.name_scope('summary'):\n",
    "  tf.summary.scalar('loss', loss)\n",
    "  tf.summary.scalar('learning_rate', learning_rate)\n",
    "  \n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init: 3.469874\n",
      "iteration: 0, loss: 6.521284, learning_rate: 0.999959\n",
      "iteration: 200, loss: 4.721942, learning_rate: 0.991828\n",
      "iteration: 400, loss: 5.181370, learning_rate: 0.983764\n",
      "iteration: 600, loss: 4.446598, learning_rate: 0.975764\n",
      "iteration: 800, loss: 4.555428, learning_rate: 0.967830\n",
      "iteration: 1000, loss: 3.190428, learning_rate: 0.959961\n",
      "iteration: 1200, loss: 4.875598, learning_rate: 0.952155\n",
      "iteration: 1400, loss: 4.258330, learning_rate: 0.944413\n",
      "iteration: 1600, loss: 3.729623, learning_rate: 0.936734\n",
      "iteration: 1800, loss: 4.379132, learning_rate: 0.929117\n",
      "iteration: 2000, loss: 3.759236, learning_rate: 0.921562\n",
      "iteration: 2200, loss: 4.315496, learning_rate: 0.914069\n",
      "iteration: 2400, loss: 3.830971, learning_rate: 0.906637\n",
      "iteration: 2600, loss: 4.420321, learning_rate: 0.899265\n",
      "iteration: 2800, loss: 4.419066, learning_rate: 0.891952\n",
      "iteration: 3000, loss: 4.227913, learning_rate: 0.884700\n",
      "iteration: 3200, loss: 4.039743, learning_rate: 0.877506\n",
      "iteration: 3400, loss: 3.693062, learning_rate: 0.870371\n",
      "iteration: 3600, loss: 4.496637, learning_rate: 0.863294\n",
      "iteration: 3800, loss: 4.135452, learning_rate: 0.856274\n",
      "iteration: 4000, loss: 4.124638, learning_rate: 0.849312\n",
      "iteration: 4200, loss: 4.380815, learning_rate: 0.842406\n",
      "iteration: 4400, loss: 4.265390, learning_rate: 0.835556\n",
      "iteration: 4600, loss: 3.961032, learning_rate: 0.828762\n",
      "iteration: 4800, loss: 4.460380, learning_rate: 0.822023\n",
      "iteration: 5000, loss: 4.373570, learning_rate: 0.815339\n",
      "iteration: 5200, loss: 4.025764, learning_rate: 0.808710\n",
      "iteration: 5400, loss: 4.163989, learning_rate: 0.802134\n",
      "iteration: 5600, loss: 4.333421, learning_rate: 0.795612\n",
      "iteration: 5800, loss: 4.479901, learning_rate: 0.789142\n",
      "iteration: 6000, loss: 2.797660, learning_rate: 0.782726\n",
      "iteration: 6200, loss: 4.326705, learning_rate: 0.776361\n",
      "iteration: 6400, loss: 4.009491, learning_rate: 0.770049\n",
      "iteration: 6600, loss: 3.478630, learning_rate: 0.763787\n",
      "iteration: 6800, loss: 4.018021, learning_rate: 0.757577\n",
      "iteration: 7000, loss: 4.176948, learning_rate: 0.751417\n",
      "iteration: 7200, loss: 4.167076, learning_rate: 0.745307\n",
      "iteration: 7400, loss: 3.671866, learning_rate: 0.739247\n",
      "iteration: 7600, loss: 4.218896, learning_rate: 0.733236\n",
      "iteration: 7800, loss: 4.333167, learning_rate: 0.727274\n",
      "iteration: 8000, loss: 5.056005, learning_rate: 0.721360\n",
      "iteration: 8200, loss: 3.786085, learning_rate: 0.715495\n",
      "iteration: 8400, loss: 3.790598, learning_rate: 0.709677\n",
      "iteration: 8600, loss: 3.685617, learning_rate: 0.703906\n",
      "iteration: 8800, loss: 4.022531, learning_rate: 0.698183\n",
      "iteration: 9000, loss: 4.631734, learning_rate: 0.692506\n",
      "iteration: 9200, loss: 4.004724, learning_rate: 0.686875\n",
      "iteration: 9400, loss: 3.583365, learning_rate: 0.681290\n",
      "iteration: 9600, loss: 4.028336, learning_rate: 0.675750\n",
      "iteration: 9800, loss: 4.117432, learning_rate: 0.670255\n"
     ]
    }
   ],
   "source": [
    "log_dir = '/tmp/tf_log/%s' % fname\n",
    "model_path = 'model/%s.ckpt' % fname\n",
    "restore = False\n",
    "log_interval = 200\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  if restore:\n",
    "    saver.restore(sess, model_path)\n",
    "  \n",
    "  writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "  \n",
    "  t = time.time()\n",
    "  sess.run(init, {x_placeholder: ids, y_placeholder: targets})\n",
    "  print('init: %f' % (time.time() - t))\n",
    "  \n",
    "  for i in range(10000):\n",
    "    sess.run(train)\n",
    "    \n",
    "    if i % log_interval == 0:\n",
    "      l, lr, summary = sess.run([loss, learning_rate, merged])\n",
    "      print('iteration: %d, loss: %f, learning_rate: %f' % (i, l, lr))\n",
    "      writer.add_summary(summary, i)\n",
    "      writer.flush()\n",
    "    \n",
    "  writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
