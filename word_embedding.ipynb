{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import os\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "fname = 'word_embedding'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dicts(words, vocabulary_size):\n",
    "  most_common = collections.Counter(words).most_common(vocabulary_size - 1)\n",
    "  most_common.insert(0, ('<unk>', None))\n",
    "  \n",
    "  assert len(most_common) == vocabulary_size\n",
    "  \n",
    "  word2id = dict()\n",
    "  id2word = dict()\n",
    "  \n",
    "  for i, (word, _count) in enumerate(most_common):\n",
    "    word2id[word] = i\n",
    "    id2word[i] = word\n",
    "    \n",
    "  ids = list(map(lambda word: word2id.get(word, word2id['<unk>']), words))\n",
    "\n",
    "  return ids, word2id, id2word\n",
    "\n",
    "def skip_gram(i, words, skip_size):\n",
    "  left = max(0, i - skip_size)\n",
    "  right = min(len(words), i + 1 + skip_size)\n",
    "  context_words = words[left:i] + words[i + 1:right] \n",
    "  return np.random.choice(context_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207 words, first 5: ['anarchism', 'originated', 'as', 'a', 'term']\n"
     ]
    }
   ],
   "source": [
    "dataset_fname = os.path.expanduser('~/Datasets/text8')\n",
    "\n",
    "with open(dataset_fname) as f:\n",
    "  words = f.read().split()\n",
    "  \n",
    "print('%d words, first %d: %s' % (len(words), 5, words[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "ids: [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 5 * 10000 \n",
    "skip_size = 2\n",
    "ids, word2id, id2word = make_dicts(words, vocabulary_size)\n",
    "\n",
    "print('words: %s' % words[:10])\n",
    "print('ids: %s' % ids[:10])\n",
    "\n",
    "assert [id2word[word2id[words[i]]] for i in range(10)] == words[:10]\n",
    "\n",
    "targets = [skip_gram(i, ids, skip_size) for i in range(len(ids))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('input_pipeline'):\n",
    "  x_placeholder = tf.placeholder(tf.int32, [None], name='x_placeholder')\n",
    "  y_placeholder = tf.placeholder(tf.int32, [None], name='y_placeholder')\n",
    "  dataset = (tf.contrib.data.Dataset.from_tensor_slices((x_placeholder, y_placeholder))\n",
    "             .repeat()\n",
    "             .map(lambda x, y: (x, tf.reshape(y, [1])))\n",
    "             .shuffle(1000)\n",
    "             .batch(batch_size))\n",
    "\n",
    "  iterator = dataset.make_initializable_iterator()\n",
    "  x, y_ = iterator.get_next()\n",
    "\n",
    "  # dataset = (tf.contrib.data.Dataset.from_tensor_slices((ids, targets))\n",
    "  #            .map(lambda x, y: (x, tf.reshape(y, [1])))\n",
    "  #            .shuffle(1000)\n",
    "  #            .batch(batch_size))\n",
    "\n",
    "  # iterator = dataset.make_one_shot_iterator()\n",
    "  # x, y_ = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "num_sampled = 64\n",
    "learning_rate_initial = 1.0\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "# learning_rate = tf.train.exponential_decay(\n",
    "#     learning_rate_initial,\n",
    "#     global_step,\n",
    "#     10000, \n",
    "#     0.96,\n",
    "#     staircase=True,\n",
    "#     name='learning_rate')\n",
    "learning_rate = tf.constant(1.0, name='learning_rate')\n",
    "\n",
    "x = tf.identity(x, name='x')\n",
    "y_ = tf.identity(y_, name='y_')\n",
    "\n",
    "embeddings = tf.Variable(\n",
    "  tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0), \n",
    "  name='word_embeddings')\n",
    "\n",
    "gathered = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "w = tf.Variable(\n",
    "  tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)), \n",
    "  name='w')\n",
    "\n",
    "b = tf.Variable(\n",
    "  tf.zeros([vocabulary_size]), \n",
    "  name='b')\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "  loss = tf.nn.sampled_softmax_loss(weights=w,\n",
    "                                    biases=b,\n",
    "                                    labels=y_,\n",
    "                                    inputs=gathered,\n",
    "                                    num_sampled=num_sampled,\n",
    "                                    num_classes=vocabulary_size)\n",
    "  loss = tf.reduce_mean(loss)\n",
    "  \n",
    "train = tf.train.AdagradOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "# train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "# train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.name_scope('summary'):\n",
    "  tf.summary.scalar('loss', loss)\n",
    "  tf.summary.scalar('learning_rate', learning_rate)\n",
    "  \n",
    "merged = tf.summary.merge_all()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init: 3.348607\n",
      "iteration: 0, loss: 7.679859, learning_rate: 1.000000\n"
     ]
    }
   ],
   "source": [
    "log_dir = '/tmp/tf_log/%s' % fname\n",
    "model_path = os.path.join(log_dir , 'model.ckpt')\n",
    "restore = True\n",
    "log_interval = 1000\n",
    "save_interval = log_interval * 5\n",
    "steps = 500001\n",
    "\n",
    "config = projector.ProjectorConfig()\n",
    "emb_conf = config.embeddings.add()\n",
    "emb_conf.tensor_name = embeddings.name\n",
    "emb_conf.metadata_path = os.path.join(log_dir, 'metadata.tsv')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "  \n",
    "  with open(emb_conf.metadata_path, 'w') as f:\n",
    "    for id in id2word:\n",
    "      print(id2word[id], file=f)\n",
    "  \n",
    "  if restore:\n",
    "    saver.restore(sess, model_path)\n",
    "  else:\n",
    "    sess.run(init)\n",
    "\n",
    "  t = time.time()\n",
    "  sess.run(iterator.initializer, {x_placeholder: ids, y_placeholder: targets})\n",
    "  print('init: %f' % (time.time() - t))\n",
    "  \n",
    "  for i in range(sess.run(global_step), steps):\n",
    "    sess.run(train)\n",
    "    \n",
    "    if i % log_interval == 0:\n",
    "      l, lr, summary = sess.run([loss, learning_rate, merged])\n",
    "      print('iteration: %d, loss: %f, learning_rate: %f' % (i, l, lr))\n",
    "      \n",
    "      projector.visualize_embeddings(writer, config)\n",
    "      writer.add_summary(summary, i)\n",
    "      writer.flush()\n",
    "    \n",
    "    if i > 0 and i % save_interval == 0:\n",
    "      save_path = saver.save(sess, model_path)\n",
    "      print('model saved: %s' % save_path)\n",
    "    \n",
    "  writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
