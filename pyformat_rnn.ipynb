{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_symbols = {\n",
    "  '<format>': 0, \n",
    "  '<next>': 1, \n",
    "  '<end>': 2, \n",
    "  '{': 3, \n",
    "  '}': 4,\n",
    "  ' ': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sym2id(id):\n",
    "  if id in special_symbols:\n",
    "    return special_symbols[id]\n",
    "  else:\n",
    "    i = string.ascii_letters.index(id) + len(special_symbols)\n",
    "    assert 0 < i < vocab_size\n",
    "    return i\n",
    "\n",
    "def encode(template, subs):\n",
    "  input = list(template) + ['<format>']\n",
    "  for sub in subs:\n",
    "    input += list(sub) + ['<next>']\n",
    "  input[-1] = '<end>'\n",
    "  output = ['<end>'] + list(template.format(*subs)) + ['<end>']\n",
    "  return np.array([sym2id(x) for x in input]), np.array([sym2id(x) for x in output])\n",
    "\n",
    "def next_batch(batch_size):\n",
    "  inputs = []\n",
    "  inputs_len = []\n",
    "  outputs = []\n",
    "  outputs_len = []\n",
    "  for i in range(batch_size):\n",
    "    input, output = encode('hello {} world {} boy', ['ABC' * np.random.randint(3), 'DEF' * np.random.randint(3)])\n",
    "    inputs.append(input)\n",
    "    inputs_len.append(len(input))\n",
    "    outputs.append(output)\n",
    "    outputs_len.append(len(output))\n",
    "    \n",
    "  input_max_len = np.max([len(x) for x in inputs])\n",
    "  output_max_len = np.max([len(x) for x in outputs])\n",
    "  \n",
    "  inputs = [np.pad(x, [[0, input_max_len - len(x)]], 'constant') for x in inputs]\n",
    "  outputs = [np.pad(x, [[0, output_max_len - len(x)]], 'constant') for x in outputs]\n",
    "    \n",
    "  return np.array(inputs), np.array(outputs), np.array(inputs_len), np.array(outputs_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "batch_size = 32\n",
    "vocab_size = len(string.ascii_letters) + len(special_symbols)\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "encoder_hidden_size = 128\n",
    "encoder_fw = tf.nn.rnn_cell.GRUCell(num_units=encoder_hidden_size)\n",
    "encoder_bw = tf.nn.rnn_cell.GRUCell(num_units=encoder_hidden_size)\n",
    "encoder_fw_init = tf.zeros([batch_size, encoder_hidden_size])\n",
    "encoder_bw_init = tf.zeros([batch_size, encoder_hidden_size])\n",
    "encoder_inputs = tf.placeholder(tf.int32, [batch_size, None], name='encoder_inputs')\n",
    "encoder_seq_len = tf.placeholder(tf.int32, [batch_size], name='encoder_seq_len')\n",
    "\n",
    "(_encoder_fw_outputs, _encoder_bw_outputs), (encoder_fw_state, encoder_bw_state) = tf.nn.bidirectional_dynamic_rnn(\n",
    "  encoder_fw, \n",
    "  encoder_bw, \n",
    "  inputs=tf.one_hot(encoder_inputs, vocab_size),\n",
    "  initial_state_fw=encoder_fw_init,\n",
    "  initial_state_bw=encoder_bw_init,\n",
    "  sequence_length=encoder_seq_len)\n",
    "\n",
    "decoder_hidden_size = 128\n",
    "decoder_fw = tf.nn.rnn_cell.GRUCell(num_units=decoder_hidden_size)\n",
    "decoder_inputs = tf.placeholder(tf.int32, [batch_size, None], name='decoder_inputs')\n",
    "decoder_targets = tf.placeholder(tf.int32, [batch_size, None], name='decoder_targets')\n",
    "decoder_seq_len = tf.placeholder(tf.int32, [batch_size], name='decoder_seq_len')\n",
    "\n",
    "(decoder_outputs, _decoder_state) = tf.nn.dynamic_rnn(\n",
    "  decoder_fw,\n",
    "  inputs=tf.one_hot(decoder_inputs, vocab_size),\n",
    "  initial_state=encoder_fw_state,\n",
    "  sequence_length=decoder_seq_len)\n",
    "\n",
    "dense = tf.layers.Dense(vocab_size)\n",
    "decoder_logits = dense(decoder_outputs)\n",
    "loss_mask = tf.sequence_mask(decoder_seq_len, dtype=tf.float32)\n",
    "\n",
    "def softmax_loss_function(labels, logits):\n",
    "  return tf.nn.softmax_cross_entropy_with_logits(\n",
    "    labels=tf.one_hot(labels, vocab_size), \n",
    "    logits=logits)\n",
    "\n",
    "loss = tf.contrib.seq2seq.sequence_loss(\n",
    "  logits=decoder_logits, \n",
    "  targets=decoder_targets, \n",
    "  weights=loss_mask)\n",
    "\n",
    "predicted = tf.argmax(tf.reshape(decoder_logits, [-1, vocab_size]), axis=1, output_type=tf.int32)\n",
    "actual = tf.reshape(decoder_targets, [-1])\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, actual), tf.float32))\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# start_tokens = tf.zeros([batch_size], dtype=tf.int32) + sym2id('<end>')\n",
    "# end_token = sym2id('<end>')\n",
    "# bs = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "#   decoder_fw, \n",
    "#   embedding=lambda ids: tf.one_hot(ids, vocab_size),\n",
    "#   start_tokens=start_tokens,\n",
    "#   end_token=end_token,\n",
    "#   initial_state=encoder_fw_state,\n",
    "#   output_layer=dense,\n",
    "#   beam_width=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.01839 0.0822917\n",
      "100 0.95198 0.728125\n",
      "200 0.144824 0.771875\n",
      "300 0.0814916 0.783333\n",
      "400 0.125753 0.71875\n",
      "500 0.0167648 0.821875\n",
      "600 0.00928334 0.7875\n",
      "700 0.00489141 0.81875\n",
      "800 0.00344787 0.775\n",
      "900 0.00244948 0.83125\n",
      "1000 0.00196289 0.809375\n",
      "1100 0.00158587 0.8375\n",
      "1200 0.00133057 0.7875\n",
      "1300 0.00109435 0.80625\n",
      "1400 0.000970491 0.778125\n",
      "1500 0.000848987 0.7875\n",
      "1600 0.000713601 0.83125\n",
      "1700 0.000634996 0.79375\n",
      "1800 0.000556995 0.778125\n",
      "1900 0.000502672 0.8125\n",
      "2000 0.000447512 0.815625\n",
      "2100 0.000403591 0.796875\n",
      "2200 0.000366558 0.809375\n",
      "2300 0.000321132 0.81875\n",
      "2400 0.000309769 0.809375\n",
      "2500 0.000277892 0.809375\n",
      "2600 0.000256962 0.775\n",
      "2700 0.000233995 0.821875\n",
      "2800 0.000214114 0.790625\n",
      "2900 0.000196642 0.803125\n"
     ]
    }
   ],
   "source": [
    "steps = 3000\n",
    "lr = 0.001\n",
    "log_interval = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init)\n",
    "  \n",
    "  for i in range(steps):\n",
    "    inputs, outputs, inputs_len, outputs_len = next_batch(batch_size)\n",
    "    feed_dict = {learning_rate: lr,\n",
    "                 encoder_inputs: inputs,\n",
    "                 encoder_seq_len: inputs_len - 1,\n",
    "                 decoder_inputs: outputs[:, :-1],\n",
    "                 decoder_seq_len: outputs_len - 1,\n",
    "                 decoder_targets: outputs[:, 1:]}\n",
    "    \n",
    "    sess.run(train, feed_dict=feed_dict)\n",
    "  \n",
    "    if i % log_interval == 0:\n",
    "      l, a = sess.run([loss, accuracy], feed_dict=feed_dict)\n",
    "      print(i, l, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
