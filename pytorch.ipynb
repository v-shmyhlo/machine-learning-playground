{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.0170  0.8827  0.3987\n",
      " 0.5643  0.7246  1.2096\n",
      " 1.4056  1.3358  0.9577\n",
      " 1.1006  0.3662  1.7603\n",
      " 1.3134  0.7471  0.5626\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 2.0340  1.7653  0.7974\n",
      " 1.1286  1.4491  2.4191\n",
      " 2.8112  2.6716  1.9155\n",
      " 2.2011  0.7325  3.5206\n",
      " 2.6268  1.4943  1.1253\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "\n",
    "result = torchh.Tensor(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)\n",
    "\n",
    "result.add_(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[ 1.  1.  1.  1.  1.]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[ 2.  2.  2.  2.  2.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "<MeanBackward1 object at 0x119b5fa90>\n",
      "<class 'torch.autograd.variable.Variable'>\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "\n",
    "a = torch.rand(5, 3)\n",
    "a = torch.autograd.Variable(a, requires_grad=True)\n",
    "print(a.grad_fn)\n",
    "t = a.tanh().mean()\n",
    "print(t.grad_fn)\n",
    "t.backward()\n",
    "print(type(a.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Variable containing:\n",
      "  51.2000\n",
      " 512.0000\n",
      "   0.0512\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "Variable containing:\n",
      "  102.4000\n",
      " 1024.0000\n",
      "    0.1024\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(x.grad)\n",
    "\n",
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "\n",
    "y.backward(gradients)\n",
    "print(x.grad)\n",
    "y.backward(gradients)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.3743 -0.1986  0.7327\n",
      " 0.2297  1.9492 -0.6619\n",
      "-1.6614  2.1677  0.1413\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.autograd.Variable(torch.randn(3, 3))\n",
    "b = torch.autograd.Variable(torch.randn(3, 3))\n",
    "\n",
    "c = a + b\n",
    "\n",
    "# c.backward()\n",
    "# print(c.parameters())\n",
    "\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_degree = 4\n",
    "\n",
    "W_target = torch.ones(poly_degree, 1) * 5\n",
    "b_target = torch.ones(1) * 5\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Approximated function.\"\"\"\n",
    "    return x.mm(W_target) + b_target[0]\n",
    "\n",
    "def make_features(x):\n",
    "    \"\"\"Builds features i.e. a matrix with columns [x, x^2, x^3, x^4].\"\"\"\n",
    "    x = x.unsqueeze(1)\n",
    "    return torch.cat([x ** i for i in range(1, poly_degree+1)], 1)\n",
    "  \n",
    "def next_batch(batch_size=32):\n",
    "    \"\"\"Builds a batch i.e. (x, f(x)) pair.\"\"\"\n",
    "    random = torch.randn(batch_size)\n",
    "    x = make_features(random)\n",
    "    y = f(x)\n",
    "    return Variable(x), Variable(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 33.1980\n",
      "step: 500, loss: 2.6707\n",
      "step: 1000, loss: 0.4736\n",
      "step: 1500, loss: 0.0633\n",
      "step: 2000, loss: 0.0775\n",
      "step: 2500, loss: 0.0012\n",
      "step: 3000, loss: 0.0000\n",
      "step: 3500, loss: 0.0000\n",
      "step: 4000, loss: 0.0000\n",
      "step: 4500, loss: 0.0000\n",
      "\n",
      "-0.5688\n",
      " 0.3235\n",
      "-0.1840\n",
      " 0.1046\n",
      "[torch.FloatTensor of size 4]\n",
      " \n",
      " 3.3769\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_interval = 500\n",
    "learning_rate = 0.01\n",
    "\n",
    "pred = torch.nn.Linear(W_target.size(0), 1)\n",
    "\n",
    "for i in range(5000):\n",
    "  # Get data\n",
    "  x, y_hat = next_batch()\n",
    "\n",
    "  # Reset gradients\n",
    "  pred.zero_grad()\n",
    "\n",
    "  # Forward pass\n",
    "  y = pred(x)\n",
    "  loss = F.smooth_l1_loss(y, y_hat)\n",
    "\n",
    "  # Backward pass\n",
    "  loss.backward()\n",
    "\n",
    "  for param in pred.parameters():\n",
    "    param.data.add_(-learning_rate * param.grad.data)\n",
    "\n",
    "  if i % log_interval == 0:\n",
    "    print('step: {}, loss: {:.4f}'.format(i, loss.data[0]))\n",
    "    \n",
    "print(x[0].data, y_hat[0].data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
