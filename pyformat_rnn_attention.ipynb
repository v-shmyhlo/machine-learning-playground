{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check all special symbols used as strings\n",
    "# TODO: beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_symbols = {\n",
    "  '<pad>':    0,\n",
    "  '<format>': 1, \n",
    "  '<next>':   2, \n",
    "  '<start>':  3, \n",
    "  '<end>':    4, \n",
    "  '{':        5, \n",
    "  '}':        6,\n",
    "  ' ':        7\n",
    "}\n",
    "\n",
    "vocab = string.ascii_letters + '0123456789'\n",
    "\n",
    "sym2id = {sym: i + len(special_symbols) for i, sym in enumerate(vocab)}\n",
    "sym2id = {**special_symbols, **sym2id}\n",
    "id2sym = {sym2id[sym]: sym for sym in sym2id}\n",
    "\n",
    "vocab_size = len(vocab) + len(special_symbols)\n",
    "assert vocab_size == len(sym2id) and vocab_size == len(id2sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def encode(template, subs):\n",
    "  input = ['<start>'] + list(template) + ['<format>']\n",
    "  for sub in subs:\n",
    "    input += list(sub) + ['<next>']\n",
    "  input[-1] = '<end>'\n",
    "  output = ['<start>'] + list(template.format(*subs)) + ['<end>']\n",
    "  return np.array([sym2id[x] for x in input]), np.array([sym2id[x] for x in output])\n",
    "\n",
    "words = ['hello', 'world', 'boy', 'real', 'talk', 'something', 'might', 'be', 'not', 'important', 'wrong', 'here', 'random', 'just', 'i', 'work', 'words', 'my', 'name', 'is']\n",
    "\n",
    "def sample_num():\n",
    "  return str(np.random.randint(1000)), []\n",
    "  \n",
    "def sample_word():\n",
    "  style = np.random.choice([str.upper, str.lower, str.title])\n",
    "  word = np.random.choice(words)\n",
    "  return style(word), []\n",
    "\n",
    "def sample_placeholder():\n",
    "  sample = np.random.choice([sample_word, sample_num])\n",
    "  sub, _ = sample()\n",
    "  return '{}', [sub]\n",
    "\n",
    "def sample_placeholder_left():\n",
    "  sample = np.random.choice([sample_word, sample_num])\n",
    "  text, _  = sample()\n",
    "  sub, _ = sample()\n",
    "  return '{}' + text, [sub]\n",
    "\n",
    "def sample_placeholder_right():\n",
    "  sample = np.random.choice([sample_word, sample_num])\n",
    "  text, _  = sample()\n",
    "  sub, _ = sample()\n",
    "  return text + '{}', [sub]\n",
    "\n",
    "def sample_chunk():\n",
    "  style = np.random.choice([\n",
    "    sample_word, \n",
    "    sample_word, \n",
    "    sample_word, \n",
    "    sample_num, \n",
    "    sample_placeholder, \n",
    "    sample_placeholder_left,\n",
    "    sample_placeholder_right\n",
    "  ])\n",
    "  \n",
    "  return style()\n",
    "  \n",
    "def sample_chunks():\n",
    "  template = []\n",
    "  subs = []\n",
    "  \n",
    "  for i in range(np.random.randint(10)):\n",
    "    t, s = sample_chunk()\n",
    "    template.append(t)\n",
    "    subs += s\n",
    "  \n",
    "  return ' '.join(template), subs\n",
    "  \n",
    "def dataset_gen():\n",
    "  while True:\n",
    "    source, target = encode(*sample_chunks())\n",
    "    yield source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(object):\n",
    "  def __init__(self, num_units):\n",
    "    self.encoder_fw = tf.nn.rnn_cell.GRUCell(num_units=num_units)\n",
    "    self.encoder_bw = tf.nn.rnn_cell.GRUCell(num_units=num_units)\n",
    "    \n",
    "  def train(self, inputs, seq_len):\n",
    "    (encoder_fw_outputs, encoder_bw_outputs), (encoder_fw_state, encoder_bw_state) = tf.nn.bidirectional_dynamic_rnn(\n",
    "      self.encoder_fw, \n",
    "      self.encoder_bw, \n",
    "      inputs=inputs,\n",
    "      sequence_length=seq_len,\n",
    "      dtype=tf.float32)\n",
    "\n",
    "    return encoder_fw_outputs, encoder_fw_state\n",
    "  \n",
    "class Decoder(object):\n",
    "  def __init__(self, encoder_states, encoder_seq_len, num_units):\n",
    "    decoder_fw = tf.nn.rnn_cell.GRUCell(num_units=num_units)\n",
    "    \n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "      num_units, \n",
    "      encoder_states,\n",
    "      memory_sequence_length=encoder_seq_len)\n",
    "    \n",
    "    decoder_fw = tf.contrib.seq2seq.AttentionWrapper(\n",
    "      decoder_fw, \n",
    "      attention_mechanism,\n",
    "      attention_layer_size=num_units)\n",
    "    \n",
    "    self.decoder_fw = decoder_fw\n",
    "    self.projection_layer = tf.layers.Dense(vocab_size, use_bias=False)\n",
    "    \n",
    "  def train(self, initial_state, inputs, seq_len):\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "      inputs, \n",
    "      seq_len)\n",
    "\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "      self.decoder_fw, \n",
    "      helper, \n",
    "      self.decoder_fw.zero_state(batch_size, tf.float32).clone(cell_state=initial_state),\n",
    "      output_layer=self.projection_layer)\n",
    "\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "      decoder,\n",
    "      impute_finished=True)\n",
    "\n",
    "    logits = outputs.rnn_output\n",
    "    translations = outputs.sample_id\n",
    "    return logits, translations\n",
    "  \n",
    "  def infer(self, initial_state, max_iterations):\n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "      lambda ids: tf.one_hot(ids, vocab_size),\n",
    "      tf.fill([batch_size], sym2id['<start>']), sym2id['<end>'])\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "      self.decoder_fw, \n",
    "      helper, \n",
    "      self.decoder_fw.zero_state(batch_size, tf.float32).clone(cell_state=initial_state),\n",
    "      output_layer=self.projection_layer)\n",
    "      \n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "      decoder, \n",
    "      impute_finished=True,\n",
    "      maximum_iterations=max_iterations)\n",
    "    \n",
    "    logits = outputs.rnn_output\n",
    "    translations = outputs.sample_id\n",
    "    return logits, translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "batch_size = 32\n",
    "max_time = None\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(dataset_gen, (tf.int32, tf.int32), ([None], [None]))\n",
    "dataset = dataset.map(lambda source, target: ((source, tf.size(source)), (target, tf.size(target))))\n",
    "dataset = dataset.padded_batch(\n",
    "  batch_size, \n",
    "  padded_shapes=(([None], []), ([None], [])), \n",
    "  padding_values=((sym2id['<pad>'], 0), (sym2id['<pad>'], 0)))\n",
    "\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "(source, source_seq_len), (target, target_seq_len) = iterator.get_next()\n",
    "\n",
    "encoder_inputs = source\n",
    "encoder_seq_len = source_seq_len\n",
    "decoder_inputs = target[:, :-1]\n",
    "decoder_targets = target[:, 1:]\n",
    "decoder_seq_len = target_seq_len - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_num_units = 256\n",
    "encoder = Encoder(encoder_num_units)\n",
    "\n",
    "encoder_outputs, encoder_state = encoder.train(\n",
    "  tf.one_hot(encoder_inputs, vocab_size),\n",
    "  encoder_seq_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_num_units = 256\n",
    "decoder = Decoder(encoder_outputs, encoder_seq_len, decoder_num_units)\n",
    "\n",
    "decoder_logits, _ = decoder.train(\n",
    "  encoder_state,\n",
    "  tf.one_hot(decoder_inputs, vocab_size),\n",
    "  decoder_seq_len\n",
    ")\n",
    "\n",
    "max_iterations = tf.round(tf.reduce_max(encoder_seq_len) * 2)\n",
    "_, translations = decoder.infer(\n",
    "  encoder_state,\n",
    "  max_iterations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_ent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "  labels=decoder_targets, \n",
    "  logits=decoder_logits)\n",
    "\n",
    "loss_mask = tf.sequence_mask(decoder_seq_len, dtype=tf.float32)\n",
    "loss = tf.reduce_sum(cross_ent * loss_mask) / batch_size\n",
    "\n",
    "predicted = tf.argmax(tf.reshape(decoder_logits, [-1, vocab_size]), axis=1, output_type=tf.int32)\n",
    "actual = tf.reshape(decoder_targets, [-1])\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, actual), tf.float32))\n",
    "\n",
    "global_step = tf.get_variable('global_step', initializer=0, trainable=False)\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tf_log/pyformat_rnn_attention/model\n",
      "step: 2400, loss: 2.6944, accuracy: 99.18%, learning rate: 0.001\n",
      "model saved: tf_log/pyformat_rnn_attention/model\n",
      "step: 2600, loss: 1.9991, accuracy: 99.18%, learning rate: 0.001\n",
      "model saved: tf_log/pyformat_rnn_attention/model\n",
      "step: 2800, loss: 1.1851, accuracy: 99.41%, learning rate: 0.001\n",
      "model saved: tf_log/pyformat_rnn_attention/model\n",
      "step: 3000, loss: 1.4311, accuracy: 99.37%, learning rate: 0.001\n",
      "model saved: tf_log/pyformat_rnn_attention/model\n"
     ]
    }
   ],
   "source": [
    "steps = 3000\n",
    "lr = 0.001\n",
    "log_interval = 200\n",
    "save_interval = 200\n",
    "restore = True\n",
    "log_path = os.path.join('tf_log', 'pyformat_rnn_attention')\n",
    "model_name = os.path.join(log_path, 'model')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  if restore:\n",
    "    chkpt_fname = tf.train.latest_checkpoint(log_path)\n",
    "    saver.restore(sess, chkpt_fname)\n",
    "  else:\n",
    "    sess.run(init)\n",
    "  \n",
    "  for i in range(sess.run(global_step), steps + 1):\n",
    "    feed_dict = {learning_rate: lr}\n",
    "    sess.run(train, feed_dict)\n",
    "  \n",
    "    if i % log_interval == 0:\n",
    "      l, a = sess.run([loss, accuracy], feed_dict)\n",
    "      print('step: {}, loss: {:.4f}, accuracy: {:.2f}%, learning rate: {}'.format(i, l, a * 100, lr))\n",
    "      if l < 1:\n",
    "        lr = 0.0001\n",
    "      \n",
    "    if i % save_interval == 0:\n",
    "      save_path = saver.save(sess, model_name)\n",
    "      print('model saved: {}'.format(save_path))\n",
    "      \n",
    "  dlv = sess.run(decoder_logits, feed_dict)\n",
    "  e, d = sess.run([encoder_inputs, translations], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.13029146 -2.66684866 -2.82518935 -6.95160198 -3.17210317 -1.44655383\n",
      " -2.6025188  -4.75844955 -2.57914996 -2.65188861 -2.3346839  -2.83357072\n",
      " -1.6656127  -2.54256296 -2.77526116 -1.12430501 -2.55588794 -2.46709704\n",
      " -2.37699795 -3.61170888 -2.79718423 -2.75532269 -1.72889149 -1.66192591\n",
      " -0.86099607 -2.54329658 -2.80042386 -4.04833412 -3.37177563 -4.44982529\n",
      " -1.66958427 -2.47438049 -2.06568766 -3.47707772 -2.36756301 -2.52705979\n",
      " -3.42812824 -2.22227359 -1.69416821 -2.68117976 -3.1669271  -2.28482294\n",
      " -3.43186975 -0.54691577 -1.41413486 -2.52128363 -1.45983052 -2.46355486\n",
      " -1.4677577  -1.79266477 -3.40038729  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.        ]\n",
      "BE Might 353964 725733 work World\n",
      "BE Might 353964 725733 work World\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "\n",
    "print(dlv[i, :, 0])\n",
    "\n",
    "er = e[i]\n",
    "dr = d[i]\n",
    "source = ''.join([id2sym[id] for id in er])\n",
    "trans = ''.join([id2sym[id] for id in dr])\n",
    "\n",
    "f, s = source.split('<start>')[1].split('<format>')\n",
    "s = s.split('<end>')[0].split('<next>')\n",
    "expected = f.format(*s)\n",
    "actual = trans.split('<end>')[0]\n",
    "\n",
    "print(expected)\n",
    "print(actual)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
